exports:
  claude: ./data/exports/claude/conversations.json
  chatgpt: ./data/exports/chatgpt/conversations.json
  #cursor: ./data/exports/cursor/conversations.json

# Embedding settings
embedding:
  provider: openai  # Options: 'local' or 'openai'
  
  # Local model settings
  local:
    model_name: all-MiniLM-L6-v2
    device: cpu  # or 'cuda' if you have GPU
  
  # OpenAI settings (only used if provider: openai)
  openai:
    model: text-embedding-3-small
    api_key_env: OPENAI_API_KEY  # Read from environment variable
    batch_size: 100  # How many to embed at once

# Storage settings
storage:
  chroma_db_path: ./data/chroma_db
  collections_metadata: ./data/collections.yaml

# Chunking settings (for long messages)
chunking:
  max_tokens: 1000  # Split messages longer than this
  overlap: 100      # Token overlap between chunks
  enabled: false    # Start with false, enable later if needed

# Ingestion settings
ingestion:
  batch_size: 100  # How many messages to process at once
  skip_empty: true # Skip messages with no content

# Topic extraction settings
topic_extraction:
  enabled: true  # Set to false to skip topic extraction
  provider: openrouter
  
  # OpenRouter settings (only used if provider: openrouter)
  openrouter:
    model: google/gemma-3-27b-it # Cheap SLM model
    api_key_env: OPENROUTER_API_KEY  # Read from environment variable
    batch_size: 10  # How many messages to process at once
    # Prompt template for extracting topics from messages
    prompt_template: |
      Extract the main topics and subtopics from this conversation message.
      Return a JSON array of topic strings, or an empty array if no clear topics.
      Only return the JSON array, nothing else.
      
      Message: {content}
